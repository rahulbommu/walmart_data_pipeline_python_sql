# üìã Quick Reference Card

**Essential Information for Walmart Data Analytics Project**

---

## üöÄ Quick Setup Commands

```bash
# Install Python dependencies
pip install pandas sqlalchemy pymysql psycopg2

# Run basic data check
python -c "import pandas as pd; df = pd.read_csv('Walmart.csv'); print(f'Data loaded: {df.shape}')"

# Start Jupyter (if installed)
jupyter notebook
```

---

## üìä Dataset Quick Facts

- **Records**: 10,051 sales transactions
- **Columns**: 11 data fields
- **Date Range**: January to March 2019
- **Stores**: Multiple Walmart branches across Texas cities
- **Categories**: Health & beauty, Electronics, Home & lifestyle, Sports & travel, Food & beverages, Fashion

**Key Columns:**
- `invoice_id` - Unique transaction ID
- `Branch` - Store identifier (WALM001, WALM002, etc.)
- `City` - Store location
- `category` - Product type
- `unit_price` - Price per item (includes $ symbol)
- `quantity` - Items purchased
- `payment_method` - Cash, Credit card, Ewallet
- `rating` - Customer satisfaction (1-10)
- `profit_margin` - Store profit percentage

---

## üóÑÔ∏è Essential SQL Queries

**Basic Data Exploration:**
```sql
SELECT COUNT(*) FROM walmart;                    -- Total records
SELECT DISTINCT category FROM walmart;           -- Product categories  
SELECT * FROM walmart LIMIT 5;                  -- Sample data
```

**Business Intelligence:**
```sql
-- Top selling categories
SELECT category, SUM(quantity) as total_sold
FROM walmart GROUP BY category ORDER BY total_sold DESC;

-- Payment method preferences  
SELECT payment_method, COUNT(*) as transactions
FROM walmart GROUP BY payment_method;

-- Average rating by category
SELECT category, AVG(rating) as avg_rating
FROM walmart GROUP BY category ORDER BY avg_rating DESC;
```

---

## üêç Essential Python Code

**Data Loading:**
```python
import pandas as pd
df = pd.read_csv('Walmart.csv')
print(df.shape)          # (10051, 11)
print(df.columns.tolist())
```

**Data Cleaning:**
```python
# Remove duplicates
df.drop_duplicates(inplace=True)

# Fix price column (remove $ symbol)
df['unit_price'] = df['unit_price'].str.replace('$', '').astype(float)

# Create calculated fields
df['total_amount'] = df['unit_price'] * df['quantity']
```

**Database Connection:**
```python
from sqlalchemy import create_engine

# SQLite (no setup required)
engine = create_engine('sqlite:///walmart.db')
df.to_sql('walmart', con=engine, if_exists='replace', index=False)

# MySQL (if installed)
engine = create_engine('mysql+pymysql://user:pass@localhost/walmart_db')
```

---

## üíº Interview Key Points

**Project Summary (30 seconds):**
"I built an end-to-end data pipeline analyzing 10,000+ Walmart sales transactions using Python and SQL. The project demonstrates ETL processes, data cleaning, and business intelligence to generate actionable insights about customer behavior, store performance, and product trends."

**Technical Highlights:**
- Data cleaning: Removed duplicates, standardized formats
- ETL pipeline: CSV ‚Üí Python processing ‚Üí Database storage
- Multi-database: Both MySQL and PostgreSQL implementations
- Business intelligence: Complex SQL queries answering real business questions

**Business Value:**
- Customer payment preferences analysis
- Store performance comparison  
- Product category profitability insights
- Customer satisfaction correlation with business metrics

**Key Statistics to Remember:**
- 10,051 total transactions analyzed
- 150+ duplicate records cleaned
- 6 product categories compared
- Multiple Texas cities analyzed
- 48% highest profit margin (Health & beauty)
- 7.2 average customer rating

---

## üîÑ Common Workflows

**Complete ETL Process:**
```
1. Extract: Load Walmart.csv with pandas
2. Transform: Clean duplicates, fix data types, create features  
3. Load: Insert into MySQL/PostgreSQL databases
4. Analyze: Run SQL queries for business insights
5. Report: Document findings and recommendations
```

**Data Quality Workflow:**
```
1. Initial exploration: df.info(), df.describe()
2. Check duplicates: df.duplicated().sum()
3. Handle missing values: df.isnull().sum()
4. Validate ranges: Check price/rating bounds
5. Fix formats: Currency symbols, date formats
```

---

## üìö Documentation Reading Order

**For Beginners:**
1. `README_FOR_BEGINNERS.md` (Start here!)
2. `COMPLETE_SETUP_AND_LEARNING_GUIDE.md` 
3. `PROJECT_GUIDE_FOR_BEGINNERS.md`
4. `DATA_ANALYTICS_TERMS_DICTIONARY.md`

**For Interview Prep:**
1. `INTERVIEW_PREPARATION_GUIDE.md`
2. `DETAILED_FILE_EXPLANATION.md`
3. Practice with `MySQL Queries.sql`

**For Technical Deep Dive:**
1. `DETAILED_FILE_EXPLANATION.md`
2. Study `project.ipynb` 
3. Analyze `MySQL Queries.sql` and `PSQL Queries.sql`

---

## üéØ Success Metrics

**You're Ready When You Can:**
- [ ] Explain what ETL means and demonstrate it
- [ ] Write basic SQL queries for business questions
- [ ] Use pandas for data cleaning and analysis
- [ ] Connect Python to databases
- [ ] Explain business value of your technical work
- [ ] Present the project confidently in 2, 5, and 10 minutes

---

## ‚ùó Troubleshooting Quick Fixes

**Python Issues:**
```bash
# Module not found
pip install --user pandas sqlalchemy

# Permission denied
pip install --user [package_name]

# Wrong Python version
python3 -m pip install [package_name]
```

**Data Issues:**
```python
# Encoding problems
df = pd.read_csv('Walmart.csv', encoding='utf-8')

# File not found
import os; print(os.getcwd())  # Check current directory

# Memory issues with large files
df = pd.read_csv('Walmart.csv', nrows=1000)  # Load subset first
```

**Database Issues:**
```python
# Use SQLite if MySQL/PostgreSQL issues
engine = create_engine('sqlite:///test.db')

# Check connection
try:
    engine.connect()
    print("Connected successfully!")
except Exception as e:
    print(f"Connection failed: {e}")
```

---

## üåü Next Steps

**Immediate:**
- Complete the project setup
- Run through all documentation
- Practice explaining to others

**Short-term:**
- Add data visualizations
- Create your own business questions
- Build a simple dashboard

**Long-term:**
- Apply to data analyst positions
- Build additional portfolio projects
- Learn advanced tools (Tableau, Power BI, machine learning)

---

**Keep this reference handy during your learning journey!** üìå

**Remember: Every expert was once a beginner. You've got this!** üí™üåü